## WiredTiger Storage Engine Support Training

### WT File Format Tour

Tour is located [here](../mdb-wt/tour.md).

### Time Series Data

Following usual sources of time-series data contain little or nothing
that is specific to WT, but of course are still relevant to diagnosing
WT issues:

* mongod logs - records slow ops as usual, and stack traces
  on assertions or segfaults that will show WT routines.

* oplog (records all operations executed, in digested form) - nothing
  WT-specific

* system.profile (when enabled) - nothing WT-specific

* config db (records sharding-related activity such as chunk moves) -
  nothing WT-specific

Following sources of time-series data can be enabled with just a
little work, either in-house when working with a repro, or by a
customer in the field when doing a repro, and provide additional
information that is specific to WT or otherwise useful in diagnosing
WT issues:

* db.serverStatus() provides quite a lot of useful information
  ([example](ex-ss.json)), include WT-specific stats. You (or a
  customer) can collect a time series of serverStatus data as follows,
  where $delay is the delay, in floating-point seconds, between
  samples:

      mongo --eval "while(true) {print(JSON.stringify(db.serverStatus())); sleep($delay*1000)}" >ss.log &

* The WT portion of the db.serverStatus() data can also be collected
  by an alternate route built into WT. You can activate this logging
  on the mongod command line, where $delay is the interval in integer
  seconds between samples. The results are a set of WiredTigerStats\*
  files in the dbpath directory.

      mongod --wiredTigerStatisticsLogDelaySecs $delay

* db.collection.stats() provides some similar information
  ([example](ex-cs.json)) on a per-collection basis, also including
  WT-specific stats. To collect a time series of collection stats for
  db $db and collection $c at interval $delay:

        mongo $db --eval "
            while(true) {
                s = db.$c.stats();
                s.time = new Date();
                print(JSON.stringify(s));
                sleep($delay*1000)
            }
        " >cs.log &

* CPU and disk time series information can be obtained using iostat as
  follows:

        iostat -t -x $delay >iostat.log &

  However this has a couple of drawbacks: it is limited to 1-second
  resolution; the timestamps don't have timezone information making
  correlation with other data sources problematic; there is some
  useful information missing, like context switch rate; and the output
  format is fragile. I've provided a tool in support-tools/timeseries
  that addresses these issues. Run it as follows, where $delay is the
  delay between samples in floating-point seconds:

        python sysmon.py $delay >sysmon.log &

* Stack trace samples. This can be useful for "advanced" analysis on
  in-house repros, and possibly for some customers on test systems,
  but is *not* suitable for use on customer production system because
  of the potential performance and functional impact on monogd. Stack
  trace samples can be collected using a tool in
  support-tools/timeseries as follows:

        python gdbmon.py $(pidof mongod) $delay >gdbmon.log &

* Future: working with development to build some portion of the above
  time series data collection into mongod as a sort of "flight data
  recorder". The data would be stored in a circular buffer (maybe a
  capped collection) of some duration for post-mortem analysis of
  customer issues.

### Data visualization

* I have been developing a [new tool](../timeseries) for graphing all
  kinds of time-series data, including all the sources mentioned
  above, in a way that aims to make finding the correlations easy. We
  will (attempt to) use it in the following exercises.

* WT comes with a graphing tool,
  [wtstats.py](https://github.com/wiredtiger/wiredtiger/blob/master/tools/wtstats.py),
  that understand the file format generated by mongod
  --wiredTigerStatisticsLogDelaySecs $delay.

### Exercises

A series of hands-on exercies based on SERVER tickets that I worked on
over recent weeks are provided.  The exercises assume the following:

* Linux machine (I'll call it "target machine"), preferably Ubuntu, to
  run the repros on - I mostly use Ubuntu on a VMware VM on my local
  workstation. Should also work with an AWS instance. My VMware
  instance has 6 cores, 8 GB memory, 50 GB disk.

* On the target machine we'll do some "advanced" exercises involving
  collecting and visualizing gdb stack traces samples. Set up (on
  Ubuntu) is as follows:

        sudo apt-get install gdb
        echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope # allows gdb to attach

* On your local workstation (where you run your browser) obtain and
  set up pre-requisites for viz tooling:

        git clone http://github.com/10gen/support-tools
        sudo pip install -f support-tools/timeseries/requirements.txt

  If you had already downloaded the support-tools repo in the past,
  please be sure to update now to pick up some recent tooling fixes.

* Chrome is preferred browser for vis tools: https://www.google.com/chrome/browser/desktop/index.html

Each exercise directory has the following structure:

* **README.md** describes the exercise, including steps to do the repro
  and collect the data to look at.

* **run.sh** is a script that you can use to run the entire repro if you
  want, starting on your local workstation. It copies the needed stuff
  to the target machine, runs the repro, copies the output files back,
  and visualizes them. Configure for your directory structure at the
  top of the script. I wrote these scripts to test the scenarios, and
  to serve as a guide if you get stuck. Feel free to use as much or as
  little of this as you want.

* **repro.sh** is the part of the repro that runs on the target machine.

* **result/** directory is what I got when I ran the repro on my
  setup. Refer to this for comparison, or if all else fails and you
  can't get the repro to work just look at this data.


#### The exercises:

* [ex1](ex1) - based on
  [CAP-1787](https://jira.mongodb.org/browse/CAP-1787) and
  [SERVER-16546](https://jira.mongodb.org/browse/SERVER-16546):
  unexpected memory growth was observed on 2.8.0-rc2 while running a
  YCSB workload.

* [ex2](ex2) - based on
  [SERVER-16247](https://jira.mongodb.org/browse/SERVER-16247) where
  we saw declining performance over time in rc1 in a replica set, but
  not standalone.

* [ex3](ex3) - based on
  [SERVER-16662](https://jira.mongodb.org/browse/SERVER-16662) (and
  several other related tickets) where we observed occasional extended
  pauses with 0 throughput under heavy load.
