# WT storage model for MongoDB by example

This document presents a tour by example of the WiredTiger storage
format as used by MongoDB.

&emsp;&emsp;1 [Record-store btree mode](#1)  
&emsp;&emsp;&emsp;&emsp;1.1 [Collection data](#1.1)  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1.1.1 [Collection btree leaf node page](#1.1.1)  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1.1.2 [Collection btree internal node page](#1.1.2)  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1.1.3 [Block manager page and extent list](#1.1.3)  
&emsp;&emsp;&emsp;&emsp;1.2 [The _id index](#1.2)  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1.2.4 [_id index btree leaf node page](#1.2.4)  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1.2.5 [_id index internal node page](#1.2.5)  
&emsp;&emsp;&emsp;&emsp;1.3 [Another index example](#1.3)  
&emsp;&emsp;&emsp;&emsp;1.4 [Comparison of WT and mmapv1 btrees](#1.4)  
&emsp;&emsp;&emsp;&emsp;1.5 [Data updates](#1.5)  
&emsp;&emsp;&emsp;&emsp;1.6 [Deletes](#1.6)  
&emsp;&emsp;&emsp;&emsp;1.7 [Large records](#1.7)  
&emsp;&emsp;&emsp;&emsp;1.8 [Large collections](#1.8)  
&emsp;&emsp;2 [LSM mode](#2)  
&emsp;&emsp;3 [Metadata files](#3)  
&emsp;&emsp;&emsp;&emsp;3.9 [WiredTiger.wt and WiredTiger.turtle](#3.9)  
&emsp;&emsp;&emsp;&emsp;3.10 [_mdb_catalog.wt](#3.10)  
&emsp;&emsp;4 [Durability](#4)  
&emsp;&emsp;5 [Checksums and compression](#5)  

**NOTE**: this is a work-in-progress and probably
contains errors as I'm writing this as I learn about WT. Please
contact me if you spot anything incorrect or questionable, or if you
have answers to any of the questions!

The examples in this document were generated by running the code and
then using
[mdb-wt](https://github.com/10gen/support-tools/tree/master/mdb-wt), a
WT version of the
[mdb](https://github.com/10gen/support-tools/tree/master/mdb) MongoDB
db dump utility.

## <a name="1"></a> 1 Record-store btree mode

The default mode for storing collections and index is record-store
btrees. This is illustrated in the following sections by a simple
example: <a name="example1"></a>

    db.c.ensureIndex({hello:1, "here's a number field":1})
    for (var i=0; i<1000; i++)
        db.c.insert({hello: 'world', "here's a number field": 12345+i})

In record-store btree mode this code creates the following files that
contain the collection and index data:

    collection-$n-$x.wt    stores data for a single collection
    index-$n-$x.wt         stores an index

Each filename follows a pattern as described above, encoding the following information:

* $n is a collection or index number, starting at 1 and incrementing
  by 1 for each collection or index
* $x **TBD**

Note that unlike mmapv1

* there is a separate file for each collection; databases play no role
  in the division of data into files.
* the db name is not encoded in the filename. The mapping from
  namespaces to filenames is maintained in a separate metadata
  collection file, \_mdb\_catalog.wt, described in a
  [later section](#metadata-files)).

**TBD**: meaning of fields of collection and index filenames.

**TBD** file size limits? multiple files for a single collection?

### <a name="1.1"></a> 1.1 Collection data

Collection data in record-store btree mode is stored in a
collection-\*.wt file. The file begins with a 4KB descriptor page,
containing only basic file information in the first few bytes:

    00000000: block_desc magic=120897(OK) major=1 minor=0 cksum=b72308d8

This is followed by a sequence of _pages_. Each page is aligned to a
4KB boundary and is a multiple of 4KB in length.

#### <a name="1.1.1"></a> 1.1.1 Collection btree leaf node page

The first page, at file offset 0x1000 (4KB) and length 0x6000 (24KB),
is a btree leaf node that contains collection data:

    00001000: page recno=0 gen=1 msz=0x5fb6 entries=654 type=7(ROW_LEAF) flags=0x4(no0)
    0000101c: block sz=0x6000 cksum=0x9288bb5a flags=0x1(cksum)
    00001028:   key desc=0x5(short) sz=1 key=pack(1)
    0000102a:   val desc=0x80(long) sz=70
    00001030:     DOC len=70
    00001031:       '_id': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 42 be =2014-11-09T13:52:34Z
    00001042:       'hello': string len=6 strlen=5 ='world'
    00001053:       "here's a number field": double 00 00 00 00 80 1c c8 40 =12345
    00001072:     EOO
    00001072:   key desc=0x5(short) sz=1 key=pack(2)
    00001074:   val desc=0x80(long) sz=70
    0000107a:     DOC len=70
    0000107b:       '_id': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 42 bf =2014-11-09T13:52:34Z
    0000108c:       'hello': string len=6 strlen=5 ='world'
    0000109d:       "here's a number field": double 00 00 00 00 00 1d c8 40 =12346
    000010bc:     EOO
    000010bc:   key desc=0x5(short) sz=1 key=pack(3)
    000010be:   val desc=0x80(long) sz=70
    000010c4:     DOC len=70
    000010c5:       '_id': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 42 c0 =2014-11-09T13:52:34Z
    000010d6:       'hello': string len=6 strlen=5 ='world'
    000010e7:       "here's a number field": double 00 00 00 00 80 1d c8 40 =12347
    00001106:     EOO
    ...

The page begins with a header. (Technically, two headers, a page
header and a block header, contributed by different software modules,
but the net effect is a single page header.) Important fields are:

* entries: number of entries on the page. Each entry is a key or a
  value, so divide by two to get the number records stored in the
  page, as each record requires a key/value pair.
* type: page type identifying this page as a leaf btree node in a
  record store.
* cksum: a checksum to detect file corruption.
* sz: page size on disk
* msz: page size in memory. Note that this is different from the size
  on disk, reflecting the fact that the in-memory format is different
  from the on-disk format, and substantial conversion is required when
  reading or writing a page.

This is followed by a sequence of key/value pairs, sorted by
key. Since this is a leaf node of a btree representing a MongoDB
collection,

* the key is an integer record id (stored as a packed int).
* the value is a BSON document which is the content of the record.

Note that unlike mmapv1, wt btrees are used for storing collection
data as well as for storing indexes.

**TBD** maximum value sizes? overflow?

**TBD** check terminology for "record id"


#### <a name="1.1.2"></a> 1.1.2 Collection btree internal node page

For this example the file continues with three more leaf node pages
for a total of four. This is then followed by an intenal btree node
page (which happens to be the root):

    00014000: page recno=0 gen=5 msz=0x57 entries=8 type=6(ROW_INT) flags=0x0()
    0001401c: block sz=0x1000 cksum=0x18eec00f flags=0x1(cksum)
    00014028:   key desc=0x5(short) sz=1 key='\x00'
    0001402a:   val desc=0x30 sz=7 addr=0,6,0x9288bb5a
    00014033:   key desc=0x9(short) sz=2 key=pack(328)
    00014036:   val desc=0x30 sz=7 addr=6,6,0x94c4fe0c
    0001403f:   key desc=0x9(short) sz=2 key=pack(655)
    00014042:   val desc=0x30 sz=7 addr=12,6,0x709db86
    0001404b:   key desc=0x9(short) sz=2 key=pack(982)
    0001404e:   val desc=0x30 sz=7 addr=18,1,0x854a24ed

Like leaf node pages this page contains a sequence of key/value
pairs. For internal nodes:

* The key is a record id that is less than or equal to the first key
  for the child page and greater than the last key on the previous
  child page. (**TBD**: check this)

* The value is an _address token_ <a name="address-token"></a>
  (**TBD** check terminology) referencing the child page, stored as a
  triple of packed ints:
    * first element appears to be page offset / 4KB - 1 (**TBD** check
      this)
    * second element appears to be page length / 4KB (**TBD** check
      this)
    * third element is checksum of referenced page

Note that unlike mmpav1 btrees, WT btrees

* are used for both collection data and indexes, and
* do not store values as such in the internal nodes, only keys and
  references to other nodes. This keeps the internal nodes compact,
  even when storing collection data.

The location of the root btree node within each collection and index
file is stored in the WiredTiger.wt file, which is described below in
the section on [Metadata Files](#metadata-files).


#### <a name="1.1.3"></a> 1.1.3 Block manager page and extent list

Our example file ends with a block manager page that contains a list
of in-use extents within this file:

    00015000: page recno=0 gen=0 msz=52(34) entries=12 type=1(BLOCK_MANAGER) flags=0()
    0001501c: block sz=4096 cksum=d228e642 flags=1(cksum)
    0001502c:   magic=71002(OK) zero=0
    00015032:   off=1000 sz=14000
    00015034:   off=0 sz=0

Each entry in the list is stored as a pair of packed ints. The list
begins and ends with sentinal entries. In this example there is a
single extent starting at offset 0x1000 (4KB) of size 0x14000 (20KB).

As we will see in a later example, there may be both in-use and unused
extents in the file; the list of unused extents is stored in a second
block manager page.

The location within each collection and index file of the block
manager pages that contain the in-use and unused extent lists is
stored in the WiredTiger.wt metadata file, which is described below in
the section on [Metadata Files](#metadata-files).

**TBD** file size limits?


### <a name="1.2"></a> 1.2 The _id index

Indexes are stored in an index-\*.wt file. In this example, the _id
index happens to be stored in the index-3-\*.wt file. This file has
the same overall structure as a collection-\*.wt file, so the file
begins with the same 4KB header containing basic file information:

    00000000: block_desc magic=120897(OK) major=1 minor=0 cksum=0xb72308d8

#### <a name="1.2.4"></a> 1.2.4 _id index btree leaf node page

This is again followed by a sequence of pages. The first page is a
btree leaf node:

    00001000: page recno=0 gen=1 msz=0x2ff6 entries=844 type=7(ROW_LEAF) flags=0x4(no0)
    0000101c: block sz=0x3000 cksum=0x144012c9 flags=0x1(cksum)
    00001028:   key desc=0x4d(short) sz=19
    0000102d:     DOC len=19
    0000102e:       '': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 42 be =2014-11-09T13:52:34Z
    0000103c:     EOO
    0000103c:   val desc=0x23(short) sz=8 val=00 00 00 00 01 00 00 00
    00001045:   key desc=0x4d(short) sz=19
    0000104a:     DOC len=19
    0000104b:       '': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 42 bf =2014-11-09T13:52:34Z
    00001059:     EOO
    00001059:   val desc=0x23(short) sz=8 val=00 00 00 00 02 00 00 00
    00001062:   key desc=0x4d(short) sz=19
    00001067:     DOC len=19
    00001068:       '': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 42 c0 =2014-11-09T13:52:34Z
    00001076:     EOO
    ...

The leaf node contains a sequence of key/value pairs (844/2 pairs in this case), where:

* The bree key is a BSON document containing fields whose names are
  the empty string, and whose values are the fields of the MongoDB
  key. In this case the _id index has keys with only one value, an
  objectid.
* The value is an 8-byte record id, which are the keys in the
  collection btree. (**TBD** exact format)

#### <a name="1.2.5"></a> 1.2.5 _id index internal node page

Here is the root node for the _id index:

    00009000: page recno=0 gen=4 msz=0x6d entries=6 type=6(ROW_INT) flags=0x0()
    0000901c: block sz=0x1000 cksum=0xb0e85b9d flags=0x1(cksum)
    00009028:   key desc=0x5(short) sz=1
    00009029:     00
    0000902a:   val desc=0x30 sz=7 addr=0,3,0x144012c9
    00009033:   key desc=0x4d(short) sz=19
    00009038:     DOC len=19
    00009039:       '': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 44 64 =2014-11-09T13:52:34Z
    00009047:     EOO
    00009047:   val desc=0x30 sz=7 addr=3,3,0x6aa87779
    00009050:   key desc=0x4d(short) sz=19
    00009055:     DOC len=19
    00009056:       '': objectid 54 5f 71 a2 fc a1 e4 46 8a 93 46 0a =2014-11-09T13:52:34Z
    00009064:     EOO
    00009064:   val desc=0x30 sz=7 addr=6,2,0x1b930774

This is a list of key/value pairs where

* The btree key is a BSON document containing fields whose names are
  the empty string, and whose values are the fields of the MongoDB
  key. In this case the _id index has keys with only one value, an
  objectid.  The key is less than or equal to the first key for the
  child page and greater than the last key on the previous child
  page. (**TBD**: check this)
* As with collection internal nodes, the value is an [address
  token](#address-token) (**TBD** check terminology) referencing the
  child page.

### <a name="1.3"></a> 1.3 Another index example

[Our example](#example1) created a second index with key {hello:1,
"here's a number field":1}. In this case this index happens to be
stored in the index-4-\*.wt file. Here's a btree leaf node for that
index:

    00001000: page recno=0 gen=1 msz=0x2ff8 entries=340 type=7(ROW_LEAF) flags=0x2(all0)
    0000101c: block sz=0x3000 cksum=0xda402a95 flags=0x1(cksum)
    00001028:   key desc=0x8d(short) sz=35
    0000102d:     DOC len=27
    0000102e:       '': string len=6 strlen=5 ='world'
    0000103a:       '': double 00 00 00 00 80 1c c8 40 =12345
    00001044:     EOO
    00001044:     00 00 00 00 01 00 00 00
    0000104c:   key desc=0x8d(short) sz=35
    00001051:     DOC len=27
    00001052:       '': string len=6 strlen=5 ='world'
    0000105e:       '': double 00 00 00 00 00 1d c8 40 =12346
    00001068:     EOO
    00001068:     00 00 00 00 02 00 00 00
    00001070:   key desc=0x8d(short) sz=35
    00001075:     DOC len=27
    00001076:       '': string len=6 strlen=5 ='world'
    00001082:       '': double 00 00 00 00 80 1d c8 40 =12347
    0000108c:     EOO
    0000108c:     00 00 00 00 03 00 00 00
    ...

Note that this node contains only keys, and no values. Each btree key
is the concatenation of

* a BSON document containing fields whose names are the empty string,
  and whose values are the fields of the MongoDB key (a string and a
  double in this case), and
* an 8-byte record id (**TBD exact format**)

Thus, whereas the _id index stores the record id as the value of a
key/value pair, this index stores the record id as part of the key,
and has no values.

**TBD** to make keys unique since this isn't a unique index? what
about non-\_id unique indexes? why not just do _id index the same way?
Because it allows for more efficient retrieval in the case of a unique
key?
    
### <a name="1.4"></a> 1.4 Comparison of WT and mmapv1 btrees

The following table summarizes the differences between mmapv1 and WT
btrees, showing the content for each item in a btree node.

|               | mmapv1 index                       | WT index                      | WT collection                   |
|---------------|------------------------------------|-------------------------------|---------------------------------|
| internal node | key, diskloc, btree node reference | key, btree node reference     | record id, btree node reference |
| leaf node     | key, diskloc, null node reference  | key, record id; _or_ key only | record id, BSON document        |

### <a name="1.5"></a> 1.5 Data updates

Suppose we now update one of our records, for example:

    db.c.update({"here's a number field": 12350}, {'now': "it's smaller"})

With mmapv1, since this update has not made the document larger, the
update would have been done in-place. However WT handles this
differently. Here is a comparison of the state of the collection file
before and after this update:

              BEFORE                               AFTER
    00001000: page entries=654 typeROW_LEAF        UNUSED
    00007000: page entries=654 type=ROW_LEAF       page entries=654 type=ROW_LEAF
    0000d000: page entries=654 type=ROW_LEAF       page entries=654 type=ROW_LEAF
    00013000: page entries=38 type=ROW_LEAF        page entries=38 type=ROW_LEAF                
    00014000: page entries=8 typeROW_INT           UNUSED
    00015000: page entries=12 type=BLOCK_MANAGER   UNUSED
    00016000:                                      page entries=654 type=ROW_LEAF
    0001c000:                                      page entries=8 type=ROW_INT
    0001d000:                                      page entries=19 type=BLOCK_MANAGER
    0001e000:                                      page entries=17 type=BLOCK_MANAGER

* The update requires updating a record contained in the btree leaf
  node that was stored in the page at 0x1000 before the update.  The
  modified leaf node is written to the end of the file at
  0x16000. This new leaf node is fully compacted; that is, the space
  allocated for the now-smaller document is just exactly enough to
  hold the document.  The original version is left in place at 0x1000,
  but will be marked as unused in the block manager record.

* Since the modified leaf node has been moved, the root node at
  0x14000 that points to the leaf node must be modified to point to
  the new location. The modified root node is written to the end of
  the file at 0x1c000.  The original version is left in place at
  0x14000, but will be marked as unused in the block manager record.

* Since the used/unused extents have changed, the block manager record
  needs to change. Two new block manager records are written to the
  end of the file, recording used and unused extents respectively.
  The previous block manager record at 0x15000 is left in place, but
  is marked unused in the new block manager records.

Here are the two new block manager records at the end of the file. The
first records the two in-use extents at 0x7000 and 0x16000:

    0001d000: page recno=0 gen=0 msz=0x3b entries=19 type=1(BLOCK_MANAGER) flags=0x0()
    0001d01c: block sz=0x1000 cksum=0x79ea689c flags=0x1(cksum)
    0001d02c:   magic=71002(OK) zero=0
    0001d032:   off=0x7000 sz=0xd000
    0001d039:   off=0x16000 sz=0x7000
    0001d03b:   off=0x0 sz=0x0

The second block manager record records the two unused extents at
0x1000 and 0x14000:

    0001e000: page recno=0 gen=0 msz=0x39 entries=17 type=1(BLOCK_MANAGER) flags=0x0()
    0001e01c: block sz=0x1000 cksum=0x342ada60 flags=0x1(cksum)
    0001e02c:   magic=71002(OK) zero=0
    0001e031:   off=0x1000 sz=0x6000
    0001e037:   off=0x14000 sz=0x2000
    0001e039:   off=0x0 sz=0x0

Now suppose we modify another leaf node, say by inserting a new document:

    db.c.insert({'this is': 'new'})

Here is a comparison of the state of the collection file
before and after this update:

              BEFORE                               AFTER
    00001000: UNUSED                               page entries=19 type=BLOCK_MANAGER
    00002000: UNUSED                               page entries=24 type=BLOCK_MANAGER
    00003000  UNUSED                               UNUSED    
    00007000: page entries=654 type=ROW_LEAF       page entries=654 type=ROW_LEAF
    0000d000: page entries=654 type=ROW_LEAF       page entries=654 type=ROW_LEAF
    00013000: page entries=38 type=ROW_LEAF        UNUSED
    00014000: UNUSED                               page entries=40 type=ROW_LEAF
    00015000: UNUSED                               page entries=8 type=ROW_INT
    00016000: page entries=654 type=ROW_LEAF       page entries=654 type=ROW_LEAF
    0001c000: page entries=8 type=ROW_INT          UNUSED
    0001d000: page entries=19 type=BLOCK_MANAGER   UNUSED
    0001e000: page entries=17 type=BLOCK_MANAGER   UNUSED

* The inserted record gets the next sequential record id, so the
  record goes into the last leaf node at 0x13000, increasing entries
  from 38 to 40 (one for key, one for value). The modified page for
  that leaf node is written to an unused location at 0x14000, and the
  previous version at 0x13000 is marked unused in the block manager
  record.

* Since the modified leaf node has a new location on disk, the root
  node at 0x1c000 that points to it must be modified. The new version
  of the root node is written to an unused location at 0x15000, and
  the previous version at 0x1c000 is marked unused in the block
  manager record.

* The new block manager records recording the new used and unused
  extents are written to unused locations at 0x1000 and 0x2000. The
  previous block manager records at 0x1d000 and 0x1e000 are marked
  unused.

Here are the new block manager records recording the unused and used
extents respectively:

    00001000: page recno=0 gen=0 msz=0x3b entries=19 type=1(BLOCK_MANAGER) flags=0x0()
    0000101c: block sz=0x1000 cksum=0xaf5837ab flags=0x1(cksum)
    0000102c:   magic=71002(OK) zero=0
    00001032:   off=0x7000 sz=0xc000
    00001039:   off=0x14000 sz=0x8000
    0000103b:   off=0x0 sz=0x0
    
    00002000: page recno=0 gen=0 msz=0x40 entries=24 type=1(BLOCK_MANAGER) flags=0x0()
    0000201c: block sz=0x1000 cksum=0xde8d1827 flags=0x1(cksum)
    0000202c:   magic=71002(OK) zero=0
    00002031:   off=0x2000 sz=0x5000
    00002037:   off=0x13000 sz=0x1000
    0000203e:   off=0x1c000 sz=0x3000
    00002040:   off=0x0 sz=0x0

Note that for purposes of illustration the preceding example was
constructed by doing a checkpoint after each update, forcing it to
reflect the updates in the on-disk data structure one at a time. In a
real application generally many updates will be written to disk at
once, resulting in less rearrangement of the data than this example
shows.

* **TBD** Are the examples shown here representative of typical behavior?
    * is it true that records are never updated in place?
    * is it always the case that when a page is changed, even if the
       page does not grow, it will not be re-written in place, but
       rather written to a currently unused extent, and the current
       location then marked as unused?
           
* **TBD** Interestingly, the block manager page listing unused extents
    lists itself as unused.

### <a name="1.6"></a> 1.6 Deletes

### <a name="1.7"></a> 1.7 Large records

The preceding examples used small records, fitting 300 or so into each
24KB page. What happens as the record size increases?

The following example was constructed by inserting increasingly large
records, starting at about 1KB and increasing by 100 bytes for each
successive record inserted:

    00000000: block_desc magic=120897(OK) major=1 minor=0 cksum=0xb72308d8
    00001000: page recno=0 gen=1 msz=0x5d43 entries=42 type=7(ROW_LEAF) flags=0x4(no0)
    0000101c: block sz=0x6000 cksum=0xf4d6c942 flags=0x1(cksum)
    00007000: page recno=0 gen=2 msz=0x5dac entries=36 type=7(ROW_LEAF) flags=0x4(no0)
    0000701c: block sz=0x6000 cksum=0x6c7a7e8 flags=0x1(cksum)
    0000d000: page recno=0 gen=3 msz=0x5de8 entries=32 type=7(ROW_LEAF) flags=0x4(no0)
    0000d01c: block sz=0x6000 cksum=0x84bfdbab flags=0x1(cksum)
    00013000: page recno=0 gen=4 msz=0x5a6a entries=28 type=7(ROW_LEAF) flags=0x4(no0)
    0001301c: block sz=0x6000 cksum=0x6df0ff7 flags=0x1(cksum)
    00019000: page recno=0 gen=5 msz=0x5ada entries=26 type=7(ROW_LEAF) flags=0x4(no0)
    0001901c: block sz=0x6000 cksum=0xa6d8bcb flags=0x1(cksum)
    0001f000: page recno=0 gen=6 msz=0x59bc entries=24 type=7(ROW_LEAF) flags=0x4(no0)
    0001f01c: block sz=0x6000 cksum=0x880a6f3 flags=0x1(cksum)
    00025000: page recno=0 gen=7 msz=0x2f0e entries=12 type=7(ROW_LEAF) flags=0x4(no0)
    0002501c: block sz=0x3000 cksum=0x89f8d5ea flags=0x1(cksum)

Note that the page size remains constant at 24KB (sz=0x6000), but each
successive page holds fewer records, starting at 21 records
(entries=42 key/value pairs) until finally the last page in the
example holds only 6 records (entries=12).

What happens if the records get even larger? Here's a similar example
where we insert successively larger records, starting at 2500 bytes
and increasing by 100 for each record inserted:

    00000000: block_desc magic=120897(OK) major=1 minor=0 cksum=0xb72308d8
    00001000: page recno=0 gen=1 msz=0xc44 entries=3100 type=5(OVFL) flags=0x0()  <-- overflow page
    0000101c: block sz=0x1000 cksum=0x11b30903 flags=0x1(cksum)
    00002000: page recno=0 gen=2 msz=0xca8 entries=3200 type=5(OVFL) flags=0x0()  <-- overflow page
    0000201c: block sz=0x1000 cksum=0xaa9ec583 flags=0x1(cksum)
    00003000: page recno=0 gen=3 msz=0xd0c entries=3300 type=5(OVFL) flags=0x0()  <-- overflow page
    0000301c: block sz=0x1000 cksum=0x4df837e9 flags=0x1(cksum)
    00004000: page recno=0 gen=4 msz=0xd70 entries=3400 type=5(OVFL) flags=0x0()  <-- overflow page
    0000401c: block sz=0x1000 cksum=0xebccfd06 flags=0x1(cksum)
    00005000: page recno=0 gen=5 msz=0x40e6 entries=20 type=7(ROW_LEAF) flags=0x4(no0)
    0000501c: block sz=0x5000 cksum=0x92d29c4e flags=0x1(cksum)
    00005028:   key desc=0x5(short) sz=1 key=pack(1)
    0000502a:   val desc=0x80(long) sz=2500                 <-- value held in btree leaf node
    000059f1:   key desc=0x5(short) sz=1 key=pack(2)
    000059f3:   val desc=0x80(long) sz=2600                 <-- value held in btree leaf node
    0000641e:   key desc=0x5(short) sz=1 key=pack(3)
    00006420:   val desc=0x80(long) sz=2700                 <-- value held in btree leaf node
    00006eaf:   key desc=0x5(short) sz=1 key=pack(4)
    00006eb1:   val desc=0x80(long) sz=2800                 <-- value held in btree leaf node
    000079a4:   key desc=0x5(short) sz=1 key=pack(5)
    000079a6:   val desc=0x80(long) sz=2900                 <-- value held in btree leaf node
    000084fd:   key desc=0x5(short) sz=1 key=pack(6)
    000084ff:   val desc=0x80(long) sz=3000                 <-- value held in btree leaf node
    000090ba:   key desc=0x5(short) sz=1 key=pack(7)
    000090bc:   val desc=0xa0 sz=7 addr=0,1,0x11b30903      <-- overflow pointer
    000090c5:   key desc=0x5(short) sz=1 key=pack(8)
    000090c7:   val desc=0xa0 sz=7 addr=1,1,0xaa9ec583      <-- overflow pointer
    000090d0:   key desc=0x5(short) sz=1 key=pack(9)
    000090d2:   val desc=0xa0 sz=7 addr=2,1,0x4df837e9      <-- overflow pointer
    000090db:   key desc=0x5(short) sz=1 key=pack(10)
    000090dd:   val desc=0xa0 sz=7 addr=3,1,0xebccfd06
    0000a000: page recno=0 gen=6 msz=0x33 entries=2 type=6(ROW_INT) flags=0x0()
    0000a01c: block sz=0x1000 cksum=0x1a5887b1 flags=0x1(cksum)
    0000b000: page recno=0 gen=0 msz=0x33 entries=11 type=1(BLOCK_MANAGER) flags=0x0()
    0000b01c: block sz=0x1000 cksum=0xd66d4ac6 flags=0x1(cksum)
    
* Focusing on the leaf page at 0x5000, we see that the first several
  records, up to 3000 bytes, are held in the leaf record as before.

* But starting with the record of size 3100, rather than storing the
  value itself in the btree node, an address token pointing to an
  overflow page is stored, and the data is stored outside the leaf
  node in the overflow page.

**TBD** Is the 24KB disk page size a parameter that can be tweaked?

**TBD** What is the threshhold for using an overflow record? Appears
to be 3KB in these examples. Tweakable?

### <a name="1.8"></a> 1.8 Large collections

Unlike mmapv1, which breaks a db up into files no larger than 2GB, WT
stores each collection in a single arbitrarily large file. The size of
the file is only limited by the amount of available disk and resource
limits like ulimit settings.

**TBD** Is this correct? No internal upper bounds? For example, the
address tokens appear to be in units of 4KB, so there is a potential
for a limit at 2^31 or 2^32 * 4KB, that is 4TB or 8TB (depending on
the internal data type used for handling the address tokens).

## <a name="2"></a> 2 LSM mode

In LSM mode the data is stored in files in the same btree format as
record-store mode; the difference is how updates to those files are
managed. The file naming scheme for LSM mode is as follows:

    collection-$n-$x-$seq.lsm    stores data for a single collection
    index-$n-$x-$seq.lsm         stores an index

Each filename follows a pattern as described above, encoding the following information:

* $n is a collection or index number, starting at 1 and incrementing
  by 1 for each collection or index
* $x **TBD**
* $seq **TBD** sequence number (*TBD* terminology?)

Note that now rather than a single file for each collection, we will
have a sequence of files for each collection, with sequential values
of the $seq part of the filename.

For example, going back to our [original example](#example1), after
initially creating the datset, we will have a single file containing
exactly the same btree data structure that we had in record-store
mode, except that it is named with a .lsm extension and has a sequence
number as part of the name:

    collection-2-3650574080730131548-000001.lsm

Then, after applying a single update to the tree, we will now have two
files:

    collection-2-3650574080730131548-000001.lsm
    collection-2-3650574080730131548-000002.lsm

The first file with the sequence number of 000001 has not been
modified. The 000002 file records the updates that need to be applied
to the 000001 file to obtain the current value of the collection. The
000002 file also a btree, in this case a very simple one containing
only one record, the one that we updated:

    00000000: block_desc magic=120897(OK) major=1 minor=0 cksum=0xb72308d8

    00001000: page recno=0 gen=1 msz=0x57 entries=2 type=7(ROW_LEAF) flags=0x4(no0)
    0000101c: block sz=0x1000 cksum=0xa2427f90 flags=0x1(cksum)
    00001028:   key desc=0x5(short) sz=1 key=pack(6)
    0000102a:   val desc=0xb3(short) sz=44
    0000102f:     DOC len=44
    00001030:       '_id': objectid 54 63 86 fa 03 85 b4 cc 9c dd 85 72 =2014-11-12T16:12:42Z
    00001041:       'now': string len=13 strlen=12 ="it's smaller"
    00001057:     EOO

    00002000: page recno=0 gen=2 msz=0x33 entries=2 type=6(ROW_INT) flags=0x0()
    0000201c: block sz=0x1000 cksum=0xf70da7b0 flags=0x1(cksum)
    00002028:   key desc=0x5(short) sz=1 key='\x00'
    0000202a:   val desc=0x30 sz=7 addr=0,1,0xa2427f90

    00003000: page recno=0 gen=0 msz=0x32 entries=10 type=1(BLOCK_MANAGER) flags=0x0()
    0000301c: block sz=0x1000 cksum=0xf2e0bde0 flags=0x1(cksum)
    0000302c:   magic=71002(OK) zero=0
    00003030:   off=0x1000 sz=0x2000
    00003032:   off=0x0 sz=0x0

In order to satisfy a query, the WT engine may need to merge the
values from multiple btree data structures in order to obtain the
latest value. Since this makes reads potentially expensive, there is a
background thread that merges the updates from multiple lsm trees and
writes them to disk, making reads faster.

**TBD** is it really as simple as (effectively) merging the trees in
all existing .lsm files for a collection, in order of sequence
number?

**TBD** when, and exactly what does the background merge thread do -
e.g. does it merge all trees?

**TBD** deletes

## <a name="3"></a> 3 Metadata files

<a name="metadata-files"></a>

The organization of metadata in WT differs substantially from the
mmapv1 storage engine:

* With mmapv1, the set of files constituting a single db is a completely
  self-contained unit. The key metadata for the db, such as the head
  of the list of extents belong to each collection and index, the head
  of the free lists, and the root of the btree for each index is
  stored in the .ns metadata file for that db. The individual files of
  the db however are not self-contained, and require the .ns file to
  place them in the context of the db. Individual collections are
  mixed arbitrarily within the db files.

* With WT, each collection and index is stored in a separate
  file. However, none of those files is self-contained, but rather
  requires metadata stored in other files as described in this section
  to locate key information such as block manager extent lists and
  btree root node.

The net result is that the whereas smallest self-contained unit under
mmapv1 from the file perspective is a db (meaning the set of files
constituting a db can be independently moved, deleted, or backed up),
this is not the case for WT: the smallest self-contained unit is an
entire dbpath for a db instance.

Here is a summary of the files and their content:

* WiredTiger.wt is a record store containing metadata about other .wt
  files, including pointers to the btree root node and extent lists in
  each .wt file.

* WiredtTiger.turtle is a text file containing metadata about
  WiredTiger.wt, including pointers to the btree root node and extent
  lists in WiredTiger.wt.

* \_mdb\_catalog.wt is a record store containing MongoDB-specific
  collection metadata. This stores the information that was stored in
  the separate .ns files unger mmapv1. This file stores the mapping
  from namespace names to .wt files.

** TBD ** The above summarizes some key information; what else should
   be included?

### <a name="3.9"></a> 3.9 WiredTiger.wt and WiredTiger.turtle

WiredTiger.wt is a record store containing metadata about the other
record stores .wt files, such as MongoDB collections and indexes. Here
for example is the set of key/value pairs for a minimal installation:

    00001000: page recno=0 gen=1 msz=0xed5 entries=24 type=7(ROW_LEAF) flags=0x4(no0) 
    0000101c: block sz=0x1000 cksum=0x36c389de flags=0x1(cksum)
    00001028:   key desc=0x59(short) sz=0x16(22) key='colgroup:_mdb_catalog\x00'
    0000103f:   val desc=0x80(long) sz=0x41(65) 
    00001082:   key desc=0xad(short) sz=0x2b(43) key='colgroup:collection-0--8112584666934280542\x00'
    000010ae:   val desc=0x80(long) sz=0x56(86) 
    00001106:   key desc=0x99(short) sz=0x26(38) key='colgroup:index-1--8112584666934280542\x00'
    0000112d:   val desc=0x80(long) sz=0x9e(158) 
    000011ce:   key desc=0x51(short) sz=0x14(20) key='colgroup:sizeStorer\x00'
    000011e3:   val desc=0xf7(short) sz=0x3d(61) 
    00001221:   key desc=0x55(short) sz=0x15(21) key='file:_mdb_catalog.wt\x00'
    00001237:   val desc=0x80(long) sz=0x288(648) 
    000014c2:   key desc=0xa9(short) sz=0x2a(42) key='file:collection-0--8112584666934280542.wt\x00'
    000014ed:   val desc=0x80(long) sz=0x288(648) 
    00001778:   key desc=0x95(short) sz=0x25(37) key='file:index-1--8112584666934280542.wt\x00'
    0000179e:   val desc=0x80(long) sz=0x2d8(728) 
    00001a79:   key desc=0x4d(short) sz=0x13(19) key='file:sizeStorer.wt\x00'
    00001a8d:   val desc=0x80(long) sz=0x27f(639) 
    00001d0f:   key desc=0x4d(short) sz=0x13(19) key='table:_mdb_catalog\x00'
    00001d23:   val desc=0x80(long) sz=0x40(64) 
    00001d65:   key desc=0xa1(short) sz=0x28(40) key='table:collection-0--8112584666934280542\x00'
    00001d8e:   val desc=0x80(long) sz=0x40(64) 
    00001dd0:   key desc=0x8d(short) sz=0x23(35) key='table:index-1--8112584666934280542\x00'
    00001df4:   val desc=0x80(long) sz=0x8d(141) 
    00001e84:   key desc=0x45(short) sz=0x11(17) key='table:sizeStorer\x00'
    00001e96:   val desc=0xfb(short) sz=0x3e(62) 

* The keys are URLs identifying a specifc resource, such as a
  collection or index record store .wt file.

* The values are ascii descriptor strings that contain information
  about that resource, for example, the pointers to the root btree
  node and extent lists for that .wt file.

This .wt file contains metadata about other .wt files; where is the
metadata about this metadata .wt file stored? It's not turtles all the
way done; in fact there is only one turtle, the WiredTiger.turtle
file, that serves as essentially the root of the whole thing. Here's
an example:

    WiredTiger version string
    WiredTiger 2.4.2: (November  6, 2014)
    WiredTiger version
    major=2,minor=4,patch=2
    file:WiredTiger.wt
    allocation_size=4KB,app_metadata=,block_allocation=best,block_compressor=,cache_resident=0,\
    checkpoint=(WiredTigerCheckpoint.1110=(addr="018981e4a8c1f5018281e4f06e3a958381e43631ac00808080e28fc0e3454fc0",\
    order=1110,time=1416002178,size=4550656,write_gen=2218)),checkpoint_lsn=(1,23032320),checksum=uncompressed,\
    collator=,columns=,dictionary=0,format=btree,huffman_key=,huffman_value=,id=0,internal_item_max=0,internal_key_truncate=,\
    internal_page_max=4KB,key_format=S,key_gap=10,leaf_item_max=0,leaf_page_max=32KB,memory_page_max=5MB,os_cache_dirty_max=0,\
    os_cache_max=0,prefix_compression=0,prefix_compression_min=4,split_pct=75,value_format=S,version=(major=1,minor=1)

This ascii WiredTiger.turtle file contains the same ascii descriptor
string that the WiredTiger.wt file contains for the other .wt
files. The pointers to the key locations in the .wt file such as btree
root node pointer and location of extent lists is encoded in the hex
string identified as "addr=...".

Note that the key landmarks within each .wt file are associated with
specific checkpoints. This means that a given .wt file may have
multiple btree root nodes, extent lists, etc., one associated with
each checkpoint. Since MongoDB only maintains one checkpoint at a time
however we can simply refer to _the_ btree root node, extent list,
etc. when talking about WT in a MongoDB context.  ** TBD ** verify
accuracy of this


### <a name="3.10"></a> 3.10 _mdb_catalog.wt

The \_mdb\_catalog file is a collection containing metadata about
collections and indexes. For example here is the entry in \_mdb\_catalog
related to our example collection:

    00001000: page recno=0 gen=1 msz=0x3a0 entries=4 type=7(ROW_LEAF) flags=0x4(no0)
    0000101c: block sz=0x1000 cksum=0xd8d426c1 flags=0x1(cksum)
    ...
    00001182:   key desc=0x5(short) sz=1 key=pack(2)
    00001184:   val desc=0x80(long) sz=537
    0000118b:     DOC len=537
    0000118c:       'md': DOC len=0x15c(348) EOO=0x12ea
    00001194:         'ns': string len=7 strlen=6 ='test.c'
    000011a3:         'options': DOC len=0x5(5) EOO=0x11af
    000011b0:         EOO
    000011b1:         'indexes': DOC len=0x131(305) EOO=0x12e9
    000011be:           '0': DOC len=0x73(115) EOO=0x1232
    000011c5:             'spec': DOC len=0x3d(61) EOO=0x1206
    000011cf:               'v': int32 01 00 00 00
    000011d6:               'key': DOC len=0xe(14) EOO=0x11e7
    000011df:                 '_id': int32 01 00 00 00
    000011e8:               EOO
    000011e9:               'name': string len=5 strlen=4 ='_id_'
    000011f8:               'ns': string len=7 strlen=6 ='test.c'
    00001207:             EOO
    00001208:             'ready': boolean 01
    00001210:             'multikey': boolean 00
    0000121b:             'head_a': int32 ff ff ff ff
    00001227:             'head_b': int32 00 00 00 00
    00001233:           EOO
    00001234:           '1': DOC len=0xb3(179) EOO=0x12e8
    0000123b:             'spec': DOC len=0x7d(125) EOO=0x12bc
    00001245:               'v': int32 01 00 00 00
    0000124c:               'key': DOC len=0x33(51) EOO=0x1282
    00001255:                 'hello': double 00 00 00 00 00 00 f0 3f =1
    00001264:                 "here's a number field": double 00 00 00 00 00 00 f0 3f =1
    00001283:               EOO
    00001284:               'name': string len=32 strlen=31 ="hello_1_here's a number field_1"
    000012ae:               'ns': string len=7 strlen=6 ='test.c'
    000012bd:             EOO
    000012be:             'ready': boolean 01
    000012c6:             'multikey': boolean 00
    000012d1:             'head_a': int32 ff ff ff ff
    000012dd:             'head_b': int32 00 00 00 00
    000012e9:           EOO
    000012ea:         EOO
    000012eb:       EOO
    000012ec:       'idxIdent': DOC len=0x6e(110) EOO=0x1362
    000012fa:         '_id_': string len=29 strlen=28 ='index-3--8114685204076478674'
    00001321:         "hello_1_here's a number field_1": string len=29 strlen=28 ='index-4--8114685204076478674'
    00001363:       EOO
    00001364:       'ns': string len=7 strlen=6 ='test.c'
    00001373:       'ident': string len=34 strlen=33 ='collection-2--8114685204076478674'
    000013a0:     EOO
    
**TBD** describe the important fields

## <a name="4"></a> 4 Durability

**TBD** when are checkpoints done?

**TBD** journal file format

## <a name="5"></a> 5 Checksums and compression
